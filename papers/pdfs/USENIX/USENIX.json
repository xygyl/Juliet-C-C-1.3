[
  {
    "file_name": "From One Thousand Pages of Specification to Unveiling Hidden Bugs_.pdf",
    "bug_type": "The bugs identified are real-world bugs. They include 5 crash bugs and 142 non-crash bugs, with 61 of them being zero-day vulnerabilities.",
    "bug_origin": "Real-world",
    "programming_language": "The paper does not explicitly mention a specific programming language used for the implementation of the tool or the devices tested. However, it discusses the Matter specification and its implementation, which typically involves languages like C/C++ for IoT devices.",
    "benchmarks_or_datasets": "The evaluation involves 23 various Matter devices, which serve as the benchmark for testing the mGPTFuzz tool.",
    "dataset_tool_repo": "Yes, the tool is available at the following link: [mGPTFuzz Repository](https://iot-fuzz.github.io).",
    "publicly_available": "Yes",
    "llm_prompt_strategy": ""
  },
  {
    "file_name": "Fuzzing BusyBox_ Leveraging LLM and Crash Reuse for Embedded Bug.pdf",
    "bug_type": "The bugs identified are real-world vulnerabilities found in older versions of BusyBox used in commercial embedded devices.",
    "bug_origin": "Real-world",
    "programming_language": "The programming language used is C, as BusyBox is primarily written in C.",
    "benchmarks_or_datasets": "The dataset used consists of BusyBox ELF binaries sourced from a proprietary firmware dataset collected by the company, which included 293 BusyBox ELF binaries across various real-world firmware binaries.",
    "dataset_tool_repo": "The text does not provide a specific link to a dataset or tool repository.",
    "publicly_available": "No",
    "llm_prompt_strategy": "Yes, the LLM prompt strategy involved using OpenAI's GPT-4 model to generate initial seeds for fuzzing. The prompt provided to the model was:"
  },
  {
    "file_name": "Exploring ChatGPT\u2019s Capabilities on Vulnerability Management.pdf",
    "bug_type": "The bug types discussed in the paper include security bug reports, which are real-world bugs. The evaluation includes both hand-crafted vulnerabilities and real-world CVEs (Common Vulnerabilities and Exposures).",
    "bug_origin": "Real-world",
    "programming_language": "The paper does not specify a particular programming language, but it refers to general software development practices and mentions code snippets, suggesting that multiple programming languages may be involved in the context of vulnerability management.",
    "benchmarks_or_datasets": "The paper mentions a dataset containing 70,346 samples and 19,355,711 tokens in total. It also references datasets used by state-of-the-art (SOTA) approaches for various vulnerability management tasks, including hand-crafted vulnerabilities and real-world CVEs.",
    "dataset_tool_repo": "Yes, the paper states that all the prompts will be provided on GitHub to support further research. The link is: [https://github.com/Jamrot/ChatGPT-Vulnerability-Management](https://github.com/Jamrot/ChatGPT-Vulnerability-Management).",
    "publicly_available": "No",
    "llm_prompt_strategy": "Yes, the paper discusses various prompt strategies, including:"
  },
  {
    "file_name": "Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities.pdf",
    "bug_type": "The bug types studied include various security vulnerabilities categorized under 25 unique vulnerability classes (CWEs). The datasets used include both synthetic (e.g., OWASP, Juliet) and real-world (e.g., CVEFixes) vulnerabilities.",
    "bug_origin": "Real-world",
    "programming_language": "The programming languages used in the study are C/C++ and Java.",
    "benchmarks_or_datasets": "The benchmarks/datasets used are:",
    "dataset_tool_repo": "The paper does not explicitly mention a link to a dataset/tool repository in the provided context.",
    "publicly_available": "No",
    "llm_prompt_strategy": "Yes, several prompting strategies are used:"
  },
  {
    "file_name": "Uncovering the Limits of Machine Learning.pdf",
    "bug_type": "The bug types discussed in the paper are related to security vulnerabilities in software. The datasets used for evaluation include real-world code snippets, specifically from open-source repositories like FFmpeg and Qemu. Therefore, the bugs are considered real-world vulnerabilities.",
    "bug_origin": "Real-world",
    "programming_language": "The programming language used in the experiments is C.",
    "benchmarks_or_datasets": "The benchmarks or datasets used in the paper include:",
    "dataset_tool_repo": "Yes, a dataset/tool repository is provided. The link is: [https://github.com/niklasrisse/USENIX_2024](https://github.com/niklasrisse/USENIX_2024).",
    "publicly_available": "Yes",
    "llm_prompt_strategy": ""
  },
  {
    "file_name": "Large Language Models for Code Analysis_ Do LLMs Really Do Their Job_.pdf",
    "bug_type": "The bug types analyzed in the paper are related to vulnerabilities in real-world programs. The dataset includes code from real-world programs (on GitHub) and their obfuscated versions.",
    "bug_origin": "Real-world",
    "programming_language": "The programming languages used in the study are JavaScript, Python, and C.",
    "benchmarks_or_datasets": "The benchmarks and datasets used include:",
    "dataset_tool_repo": "Yes, a dataset/tool repository is provided. The link is: [https://github.com/aseec-lab/llms-for-code-analysis](https://github.com/aseec-lab/llms-for-code-analysis).",
    "publicly_available": "Yes",
    "llm_prompt_strategy": ""
  },
  {
    "file_name": "An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models_.pdf",
    "bug_type": "The bug type is a backdoor attack on code completion models. The attacks are primarily synthetic as they involve the deliberate injection of malicious payloads into the training data.",
    "bug_origin": "Synthetic",
    "programming_language": "The programming language used is Python.",
    "benchmarks_or_datasets": "The datasets used include a collection of Python code files harvested from GitHub repositories tagged with 'Python' and having 100+ stars from 2017 to 2022. The dataset was refined to approximately 8 GB of Python code, comprising 1,080,606 files, partitioned into three subsets for different purposes.",
    "dataset_tool_repo": "The context does not provide a specific link to a dataset/tool repository. It mentions that the dataset was collected from GitHub but does not specify a repository link.",
    "publicly_available": "No",
    "llm_prompt_strategy": "Yes, the paper describes a prompt strategy for LLMs (specifically GPT-4) to transform and obfuscate payloads. The prompts are designed to guide the LLM in modifying the original payloads to evade static analysis rules while keeping the code vulnerable for testing purposes. The prompts utilize a combination of role prompts and instruction prompts to enhance the model's ability to generate targeted outputs."
  },
  {
    "file_name": "PENTESTGPT_ Evaluating and Harnessing Large Language Models for Automated.pdf",
    "bug_type": "The bug types addressed in the study are real-world vulnerabilities, specifically those listed in OWASP's top 10 vulnerability list and 18 Common Weakness Enumeration (CWE) items.",
    "bug_origin": "Real-world",
    "programming_language": "The programming language used in the implementation of PENTESTGPT is Python.",
    "benchmarks_or_datasets": "The benchmarks used include test machines from HackTheBox and VulnHub, comprising 13 targets with 182 sub-tasks that cover vulnerabilities from OWASP's top 10 and various CWE items.",
    "dataset_tool_repo": "Yes, the tool/benchmark is publicly available on GitHub. The link is [GitHub Repository](https://github.com/your-repo-link) (Note: The actual link was not provided in the context; please replace with the correct link if available).",
    "publicly_available": "Yes",
    "llm_prompt_strategy": "Yes, the study employs a prompt strategy that includes interactive and iterative prompting, as well as the Chain-of-Thought (CoT) methodology to guide the LLMs through the penetration testing process. The prompts are designed to break down tasks into micro-steps and provide examples to enhance the LLMs' reasoning capabilities."
  },
  {
    "file_name": "SECURITYNET_ Assessing Machine Learning Vulnerabilities on Public Models.pdf",
    "bug_type": "The bug types discussed in the paper include membership inference attacks, model stealing attacks, and backdoor detection. These are considered real-world vulnerabilities as they pertain to actual security and privacy risks in machine learning models.",
    "bug_origin": "Real-world",
    "programming_language": "The paper does not explicitly mention the programming language used for the implementation of the models or experiments, but it references PyTorch's official torchvision library, which indicates that Python is likely the primary programming language used.",
    "benchmarks_or_datasets": "The benchmarks or datasets mentioned in the paper include CIFAR-10, CIFAR-100, SVHN, and ImageNet-1k. The paper also refers to a large-scale dataset of public models called SECURITYNET, which comprises models trained on 42 different datasets.",
    "dataset_tool_repo": "Yes, the paper mentions that they plan to share SECURITYNET with the research community, but it does not provide a specific link in the provided context. The repository is intended to facilitate research in machine learning security and privacy.",
    "publicly_available": "No",
    "llm_prompt_strategy": "The context does not mention any specific LLM (Large Language Model) prompt strategy used in the research. The focus is primarily on evaluating machine learning vulnerabilities and attacks rather than on prompt strategies for language models."
  },
  {
    "file_name": "EaTVul_ ChatGPT-based Evasion Attack Against Software Vulnerability Detection.pdf",
    "bug_type": "The bug types discussed in the paper include various vulnerabilities such as buffer errors (CWE119), resource management errors (CWE399), and use-after-free vulnerabilities (CWE416). The vulnerabilities are derived from both real-world projects (like Asterisk and OpenSSL) and synthetic datasets.",
    "bug_origin": "Real-world",
    "programming_language": "The programming languages used in the experiments are primarily C and C++, with extensions to Java for generalizability testing.",
    "benchmarks_or_datasets": "The datasets mentioned include:",
    "dataset_tool_repo": "Yes, a dataset/tool repository is provided. The link is: [https://github.com/wolong3385/EatVul-Resources](https://github.com/wolong3385/EatVul-Resources).",
    "publicly_available": "Yes",
    "llm_prompt_strategy": ""
  }
]